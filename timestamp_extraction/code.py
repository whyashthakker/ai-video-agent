from dotenv import load_dotenv
import ffmpeg
from faster_whisper import WhisperModel
import logging
from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip
from timestamp_extraction.captions import captions, watermark_captions
import numpy as np
import traceback
from openai import OpenAI
import os

load_dotenv()

openai_api_key = os.environ.get("OPENAI_API_KEY")


def apply_watermarks(video, watermark, brand_watermark):
    if watermark:
        watermark_clip = generate_watermark_clip(video)
        video = CompositeVideoClip([video, watermark_clip])

    if brand_watermark and brand_watermark.strip():
        brand_watermark_clip = add_brand_watermark(video, brand_watermark)
        video = CompositeVideoClip([video, brand_watermark_clip])

    return video


def extract_audio_from_video(temp_dir, output_video_file):
    audiofilename = output_video_file.replace(".mp4", ".mp3")

    retries = 3
    while retries:
        try:
            # Create the ffmpeg input stream
            input_stream = ffmpeg.input(output_video_file)

            # Extract the audio stream from the input stream
            audio = input_stream.audio

            # Save the audio stream as an MP3 file
            output_stream = ffmpeg.output(audio, audiofilename)

            # Overwrite output file if it already exists
            output_stream = ffmpeg.overwrite_output(output_stream)

            ffmpeg.run(output_stream)

            return audiofilename

        except Exception as e:
            logging.error(f"Failed to extract audio due to error: {e}. Retrying...")
            retries -= 1
    if retries == 0:
        logging.error(f"Failed to extract audio after 3 attempts.")
        return None


def transcribe_audio_with_openai(audiofilename):
    client = OpenAI(api_key=openai_api_key)

    retries = 3
    while retries:
        try:
            with open(audiofilename, "rb") as audio_file:
                transcript = client.audio.transcriptions.create(
                    file=audio_file,
                    model="whisper-1",
                    response_format="verbose_json",
                    timestamp_granularities=["word"],
                )

                # print(transcript)

            wordlevel_info = transcript.words

            # wordlevel_info = [
            #     {"word": word.word, "start": word.start, "end": word.end}
            #     for word in transcript.words
            # ]

            return wordlevel_info
        except Exception as e:
            logging.error(f"Failed to transcribe audio due to error: {e}. Retrying...")
            retries -= 1

    if retries == 0:
        logging.error(f"Failed to transcribe audio after 3 attempts.")
        return []


def transcribe_audio(audiofilename):
    model_size = "medium"
    retries = 3
    while retries:
        try:
            model = WhisperModel(model_size)

            segments, info = model.transcribe(audiofilename, word_timestamps=True)
            segments = list(segments)  # The transcription will actually run here.
            # for segment in segments:
            #     for word in segment.words:
            #         print("[%.2fs -> %.2fs] %s" % (word.start, word.end, word.word))

            wordlevel_info = []
            for segment in segments:
                for word in segment.words:
                    wordlevel_info.append(
                        {"word": word.word, "start": word.start, "end": word.end}
                    )

            return wordlevel_info
        except Exception as e:
            logging.error(f"Failed to transcribe audio due to error: {e}. Retrying...")
            retries -= 1

    if retries == 0:
        logging.error(f"Failed to transcribe audio after 3 attempts.")
        return []


def generate_watermark_clip(video):
    # Define the watermark text properties
    watermark_text = "Generated by Snapy.ai"
    font_size = 40
    color = "white"
    bg_color = "transparent"
    position = ("center", "bottom")

    # Create the watermark TextClip
    watermark_clip = (
        TextClip(watermark_text, font_size=font_size, color=color, bg_color=bg_color)
        .with_position(position)
        .with_duration(video.duration)
    ).margin(bottom=100, opacity=0)

    return watermark_clip


def add_brand_watermark(video, brand_watermark):
    # Define the watermark text properties
    watermark_text = brand_watermark
    font_size = 40
    color = "white"
    bg_color = "transparent"
    position = ("center", "top")

    # Create the watermark TextClip
    watermark_clip = (
        TextClip(watermark_text, font_size=font_size, color=color, bg_color=bg_color)
        .with_position(position)
        .with_duration(video.duration)
    ).margin(top=50, opacity=0)

    return watermark_clip


def generate_final_video(
    wordlevel_info,
    output_video_file,
    current_foldername,
    watermark=True,
    brand_watermark: str = None,
    text_effect: str = None,
    caption_id="4",
    subtitle_position="center",
):
    retries = 3
    while retries:
        try:
            video = VideoFileClip(output_video_file)

            clips = [
                generate_text_clip(
                    item["word"],
                    item["start"],
                    item["end"],
                    video,
                    text_effect=text_effect,
                    caption_id=caption_id,
                    subtitle_position=subtitle_position,
                )
                for item in wordlevel_info
            ]

            if watermark:
                watermark_clip = generate_watermark_clip(video)
                clips.append(watermark_clip)

            if brand_watermark and brand_watermark.strip():
                brand_watermark_clip = add_brand_watermark(video, brand_watermark)
                clips.append(brand_watermark_clip)

            final_video = CompositeVideoClip([video] + clips)
            final_video = final_video.with_audio(video.audio)

            finalvideoname = current_foldername + "/" + "final.mp4"
            final_video.write_videofile(
                finalvideoname, codec="libx264", audio_codec="aac"
            )

            logging.info("Video saved as '%s'", finalvideoname)

            return finalvideoname
        except Exception as e:
            logging.error(
                f"Failed to generate final video due to error: {e}. Retrying..."
            )
            retries -= 1

    if retries == 0:
        logging.error(f"Failed to generate final video after 3 attempts.")
        return None


def apply_bouncy_effect(clip, duration, video, subtitle_position="center"):
    """Bouncy animation effect."""
    amplitude = 30  # Adjust for more/less bounce
    frequency = 5.0  # Adjust for faster/slower bounce

    # Determine vertical start position based on subtitle_position
    if subtitle_position == "center":
        vertical_position = video.size[1] / 2
    elif subtitle_position == "top":
        vertical_position = video.size[1] / 4
    else:  # "bottom"
        vertical_position = 3 * video.size[1] / 4

    def bouncy_position(t):
        # Create a damping factor that diminishes amplitude over time
        damping = 1 - t / duration
        return (
            "center",
            vertical_position - damping * amplitude * np.abs(np.sin(frequency * t)),
        )

    return clip.with_position(bouncy_position).with_duration(duration)


# def apply_pop_effect(clip, duration):

#     """Pop effect animation."""

#     max_scale = 1.2  # Adjust for maximum scale value during pop
#     original_width = clip.w
#     original_height = clip.h

#     print(f"Duration: {duration}")  # <-- Diagnostic print

#     def scale_function(t):
#         scale = 1 + (max_scale - 1) * np.sin(np.pi * t / duration)
#         print(f"Scale at t={t}: {scale}")  # <-- Diagnostic print
#         return scale

#     def resize_function(t):
#         current_scale = scale_function(t)
#         return (int(original_width * current_scale), int(original_height * current_scale))

#     return clip.resize(resize_function).with_duration(duration)


def generate_text_clip(
    word,
    start,
    end,
    video,
    text_effect=None,
    caption_id="4",
    subtitle_position="center",
):
    retries = 3
    while retries:
        try:
            # Define the template
            caption = captions[caption_id]

            if caption["casing"] == "lower":
                word = word.lower()
            else:
                word = word.upper()

            # Dynamic Font Sizing
            if caption["font_size"] == "dynamic":
                if len(word) <= 5:
                    font_size = 100
                elif len(word) <= 10:
                    font_size = 90
                else:
                    font_size = 80
            else:
                font_size = int(caption["font_size"])  # Ensure font_size is an integer

            # Define desired padding and text size
            padding = 20

            text_size = TextClip(
                word, font_size=80, color="black", font=caption["font"]
            ).size
            clip_size = (text_size[0] + 2.5 * padding, text_size[1] + 2.5 * padding)

            # Construct TextClip parameters
            clip_params = {
                "text": word,
                "font_size": font_size,
                "color": caption["color"],
                "font": caption["font"],
                "size": clip_size,
            }

            # Conditionally add bg_color, stroke_color, and stroke_width
            if "bg_color" in caption:
                clip_params["bg_color"] = caption["bg_color"]
            if "stroke_color" in caption:
                clip_params["stroke_color"] = caption["stroke_color"]
            if "stroke_width" in caption:
                clip_params["stroke_width"] = caption["stroke_width"]

            if subtitle_position:
                subtitle_position = subtitle_position
            else:
                subtitle_position = "center"

            # Generate the TextClip with the adjusted size and position
            txt_clip = TextClip(**clip_params)

            txt_clip = txt_clip.with_position(("center", "center")).with_duration(
                end - start
            )

            # If shadow is present in the caption, generate a shadow TextClip
            if "shadow_color" in caption and "shadow_offset" in caption:
                shadow_params = clip_params.copy()
                shadow_params["color"] = caption["shadow_color"]
                shadow_clip = TextClip(**shadow_params)
                shadow_offset_x, shadow_offset_y = caption["shadow_offset"]

                # Calculate the shadow's position based on the main text position and shadow_offset
                shadow_position = (
                    video.size[0] / 2 - clip_size[0] / 2 + shadow_offset_x,
                    video.size[1] / 2 - clip_size[1] / 2 + shadow_offset_y,
                )

                shadow_clip = shadow_clip.with_position(shadow_position).with_duration(
                    end - start
                )

                # Composite the shadow and main text
                txt_clip = CompositeVideoClip([shadow_clip, txt_clip])

                # txt_clip = apply_pop_effect(txt_clip, end-start)

                # Now, let's position the main text in relation to the video
                txt_clip = txt_clip.with_position(
                    ("center", subtitle_position)
                ).with_duration(end - start)

                # Set margin depending on subtitle_position
                if subtitle_position == "top":
                    txt_clip = txt_clip.margin(top=150, opacity=0)
                elif subtitle_position == "bottom":
                    txt_clip = txt_clip.margin(bottom=150, opacity=0)

                txt_clip = txt_clip.with_position(("center", subtitle_position))

            else:
                # txt_clip = apply_bouncy_effect(txt_clip, end - start, video)
                # txt_clip = apply_pop_effect(txt_clip, end-start)

                # If there's no shadow, just position the main text
                txt_clip = txt_clip.with_position(
                    ("center", subtitle_position)
                ).with_duration(end - start)

                # Set margin depending on subtitle_position
                if subtitle_position == "top":
                    txt_clip = txt_clip.margin(top=150, opacity=0)
                elif subtitle_position == "bottom":
                    txt_clip = txt_clip.margin(bottom=150, opacity=0)

            # Fade in and out animation
            if "fade_in" in caption:
                txt_clip = txt_clip.crossfadein(caption["fade_in"])
            if "fade_out" in caption:
                txt_clip = txt_clip.crossfadeout(caption["fade_out"])

            if text_effect:
                txt_clip = apply_bouncy_effect(
                    txt_clip, end - start, video, subtitle_position=subtitle_position
                )

            return txt_clip.with_start(start)

        except Exception as e:
            logging.error(f"Fail to generate Text clip: {e}. Retrying...")
            logging.error(traceback.format_exc())  # <-- Add this line
            retries -= 1

    if retries == 0:
        logging.error(f"Failed to generate Text Clip after 3 attempts.")
        return None
